{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUQOx0a4o6xq/unI4Pp2d4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linbinbin/Langchain_practice/blob/main/colab/Langchain_structout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N_CDX0RYo--"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install langchain-community\n",
        "!pip install langgraph\n",
        "!pip install langchain-anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "#os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('CLAUDE_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['LANGCHAIN_PROJECT'] = \"claude_01\"\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
        "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    temperature=0,\n",
        "    max_tokens=4096,\n",
        "    timeout=None,\n",
        "    max_retries=3,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "#model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
      ],
      "metadata": {
        "id": "9zcbAid-a0i6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
        "\n",
        "\n",
        "structured_llm = llm.with_structured_output(Joke)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "id": "Dl_17z8SAp3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a3e69b-9be9-415c-edb3-de19ba3a1610"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Joke(setup='Why did the cat cross the road?', punchline='To get to the other side!', rating=6)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_schema = {\n",
        "    \"title\": \"joke\",\n",
        "    \"description\": \"Joke to tell user.\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"setup\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The setup of the joke\",\n",
        "        },\n",
        "        \"punchline\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The punchline to the joke\",\n",
        "        },\n",
        "        \"rating\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"setup\", \"punchline\"],\n",
        "}\n",
        "structured_llm = llm.with_structured_output(json_schema)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRz95fKqM9VL",
        "outputId": "ac3ea339-dfad-45a0-8a4c-1bd1e130d41e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Why did the cat cross the road?',\n",
              " 'punchline': 'To get to the other side!',\n",
              " 'rating': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing between multiple schemas\n",
        "The simplest way to let the model choose from multiple schemas is to create a parent Pydantic class that has a Union-typed attribute:"
      ],
      "metadata": {
        "id": "N7hvedUrjY0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "\n",
        "class ConversationalResponse(BaseModel):\n",
        "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
        "    response: str = Field(description=\"A conversational response to the user's query\")\n",
        "\n",
        "\n",
        "class Response(BaseModel):\n",
        "    output: Union[Joke, ConversationalResponse]\n",
        "\n",
        "\n",
        "structured_llm = llm.with_structured_output(Response)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2gtfLaNNdVp",
        "outputId": "af388fa4-2f7e-40f2-dc84-da90103c4dd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(output=Joke(setup='Why are cats good at video games?', punchline=\"Because they're purr-fect!\", rating=7))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm.invoke(\"How are you today?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qukieHUTPn1G",
        "outputId": "d72ff2aa-6103-4991-a48d-53a441a2e6db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(output=ConversationalResponse(response=\"I'm doing well, thank you for asking! How are you doing today?\"))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "    name: str = Field(..., description=\"The name of the person\")\n",
        "    height_in_meters: float = Field(\n",
        "        ..., description=\"The height of the person expressed in meters.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "    people: List[Person]\n",
        "\n",
        "\n",
        "# Set up a parser\n",
        "parser = PydanticOutputParser(pydantic_object=People)\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ").partial(format_instructions=parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "Q0lLNe1w2CeO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
        "\n",
        "print(prompt.invoke(query).to_string())\n",
        "chain = prompt | llm\n",
        "\n",
        "ret = chain.invoke({\"query\": query})\n",
        "print(ret.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEmWHXgQ2IZ3",
        "outputId": "5c2c84c5-f2ff-43f6-d718-9fea9a90d2e4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer the user query. Wrap the output in `json` tags\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\n",
            "```\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n",
            "Here is the output formatted as a JSON instance:\n",
            "\n",
            "{\n",
            "  \"people\": [\n",
            "    {\n",
            "      \"name\": \"Anna\",\n",
            "      \"height_in_meters\": 1.83\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}